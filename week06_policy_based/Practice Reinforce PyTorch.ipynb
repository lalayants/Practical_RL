{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"N72QAMfO00OO"},"source":["# REINFORCE in pytorch\n","\n","Just like we did before for q-learning, this time we'll design a pytorch network to learn `CartPole-v0` via policy gradient (REINFORCE).\n","\n","Most of the code in this notebook is taken from approximate qlearning, so you'll find it more or less familiar and even simpler."]},{"cell_type":"code","execution_count":45,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"colab_type":"code","executionInfo":{"elapsed":1793,"status":"ok","timestamp":1590104741108,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"},"user_tz":300},"id":"OosJvRfA00OR","outputId":"644d13d6-2bc8-4e01-e9bd-8e53ee4dc4a4"},"outputs":[],"source":["import sys, os\n","if 'google.colab' in sys.modules and not os.path.exists('.setup_complete'):\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/spring20/setup_colab.sh -O- | bash\n","\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/grading.py -O ../grading.py\n","    !wget -q https://raw.githubusercontent.com/yandexdataschool/Practical_RL/coursera/week5_policy_based/submit.py\n","\n","    !touch .setup_complete\n","\n","# This code creates a virtual display to draw game images on.\n","# It will have no effect if your machine has a monitor.\n","if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\")) == 0:\n","    !bash ../xvfb start\n","    os.environ['DISPLAY'] = ':1'"]},{"cell_type":"code","execution_count":46,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":287},"colab_type":"code","executionInfo":{"elapsed":1179,"status":"ok","timestamp":1590104742753,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"},"user_tz":300},"id":"-Vid5Ljs00Of","outputId":"3db7132e-7950-4c53-e03e-a4b011c24512"},"outputs":[],"source":["import gymnasium as gym\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n","\n","# gym compatibility: unwrap TimeLimit\n","if hasattr(env, '_max_episode_steps'):\n","    env = env.env\n","\n","env.reset()\n","n_actions = env.action_space.n\n","state_dim = env.observation_space.shape"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kxZxrv-100Os"},"source":["# Building the network for REINFORCE"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XupWrlwE00Ou"},"source":["For REINFORCE algorithm, we'll need a model that predicts action probabilities given states. Let's define such a model below."]},{"cell_type":"code","execution_count":47,"metadata":{"colab":{},"colab_type":"code","id":"wMm4rRI600Ov"},"outputs":[],"source":["import torch\n","import torch.nn as nn"]},{"cell_type":"code","execution_count":48,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":141},"colab_type":"code","executionInfo":{"elapsed":3514,"status":"ok","timestamp":1590104748754,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"},"user_tz":300},"id":"ac0pIHnT00O4","outputId":"2277b791-6a9e-4ec6-e701-67c7683f69f1"},"outputs":[{"data":{"text/plain":["Sequential(\n","  (0): Linear(in_features=4, out_features=256, bias=True)\n","  (1): ReLU()\n","  (2): Linear(in_features=256, out_features=128, bias=True)\n","  (3): ReLU()\n","  (4): Linear(in_features=128, out_features=2, bias=True)\n",")"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["# Build a simple neural network that predicts policy logits. \n","# Keep it simple: CartPole isn't worth deep architectures.\n","model = nn.Sequential(\n","  nn.Linear(in_features=4, out_features=256, bias=True)\n","  , nn.ReLU()\n","  , nn.Linear(in_features=256, out_features=128, bias=True)\n","  , nn.ReLU()\n","  , nn.Linear(in_features=128, out_features=n_actions, bias=True)\n",")\n","cuda = torch.device('cuda') \n","model.cuda()"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"fvfC2FPU00PB"},"source":["#### Predict function"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"JHmT7P5N00PD"},"source":["Note: output value of this function is not a torch tensor, it's a numpy array.\n","So, here gradient calculation is not needed.\n","<br>\n","Use [no_grad](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n","to suppress gradient calculation.\n","<br>\n","Also, `.detach()` (or legacy `.data` property) can be used instead, but there is a difference:\n","<br>\n","With `.detach()` computational graph is built but then disconnected from a particular tensor,\n","so `.detach()` should be used if that graph is needed for backprop via some other (not detached) tensor;\n","<br>\n","In contrast, no graph is built by any operation in `no_grad()` context, thus it's preferable here."]},{"cell_type":"code","execution_count":49,"metadata":{"colab":{},"colab_type":"code","id":"bwHUIiYH00PF"},"outputs":[],"source":["from scipy.special import softmax\n","#torch.set_default_tensor_type('torch.cuda.FloatTensor')\n","\n","@torch.no_grad()\n","def predict_probs(states):\n","    \"\"\" \n","    Predict action probabilities given states.\n","    :param states: numpy array of shape [batch, state_shape]\n","    :returns: numpy array of shape [batch, n_actions]\n","    \"\"\"\n","    # convert states, compute logits, use softmax to get probability\n","    \"<YOUR CODE>\"\n","    n = states.shape[0]\n","    states = torch.tensor(states, dtype=torch.float32, device=cuda)\n","    qvalues = model(states) \n","    qvalues_proba = nn.functional.softmax(qvalues, dim=1)\n","\n","    \n","    return qvalues_proba.cpu().detach().numpy()"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"colab_type":"code","executionInfo":{"elapsed":287,"status":"ok","timestamp":1590104752718,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"},"user_tz":300},"id":"wP4UjSrSHAkI","outputId":"3cf6466c-45d9-470b-83ae-bdeda9b6553a"},"outputs":[{"name":"stdout","output_type":"stream","text":["(array([0.03570739, 0.02011935, 0.03812407, 0.0411163 ], dtype=float32), {})\n"]}],"source":["print(env.reset())"]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{},"colab_type":"code","id":"0eAy35AA00PM"},"outputs":[],"source":["test_states = np.array([env.reset()[0] for _ in range(5)])\n","test_probas = predict_probs(test_states)\n","assert isinstance(\n","    test_probas, np.ndarray), \"you must return np array and not %s\" % type(test_probas)\n","assert tuple(test_probas.shape) == (\n","    test_states.shape[0], env.action_space.n), \"wrong output shape: {}\".format(np.shape(test_probas))\n","assert np.allclose(np.sum(test_probas, axis=1),\n","                   1), \"probabilities do not sum to 1\""]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"SbCB1_DX00PT"},"source":["### Play the game\n","\n","We can now use our newly built agent to play the game."]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{},"colab_type":"code","id":"VjhzWFjs00PV"},"outputs":[],"source":["def generate_session(env, t_max=1000):\n","    \"\"\" \n","    play a full session with REINFORCE agent and train at the session end.\n","    returns sequences of states, actions andrewards\n","    \"\"\"\n","    # arrays to record session\n","    states, actions, rewards = [], [], []\n","    s = env.reset()[0]\n","\n","    for t in range(t_max):\n","        # action probabilities array aka pi(a|s)\n","        action_probs = predict_probs(np.array([s]))[0]\n","\n","        # Sample action with given probabilities.\n","        \"<YOUR CODE>\"\n","        a = np.random.choice((0, 1), p=action_probs)\n","        new_s, r, terminated, truncated, info = env.step(a)\n","\n","        # record session history to train later\n","        states.append(s)\n","        actions.append(a)\n","        rewards.append(r)\n","\n","        s = new_s\n","        if terminated or truncated:\n","            break\n","\n","    return states, actions, rewards"]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{},"colab_type":"code","id":"GXIJO4XZ00PZ"},"outputs":[],"source":["# test it\n","states, actions, rewards = generate_session(env)\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"N8nl96pb00Ph"},"source":["### Computing cumulative rewards"]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{},"colab_type":"code","id":"h4cTvReR00Pi"},"outputs":[],"source":["def get_cumulative_rewards(rewards,  # rewards at each step\n","                           gamma=0.99  # discount for reward\n","                           ):\n","    \"\"\"\n","    take a list of immediate rewards r(s,a) for the whole session \n","    compute cumulative returns (a.k.a. G(s,a) in Sutton '16)\n","    G_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n","\n","    The simple way to compute cumulative rewards is to iterate from last to first time tick\n","    and compute G_t = r_t + gamma*G_{t+1} recurrently\n","\n","    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n","    \"\"\"\n","    # <YOUR CODE>\n","    l = len(rewards)\n","    G = list(range(l))\n","\n","    G[l-1] = rewards[-1]\n","\n","    for t in reversed(range(l-1)):\n","      G[t] = rewards[t] + gamma * G[t+1]\n","   \n","  \n","    return G"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"colab_type":"code","executionInfo":{"elapsed":392,"status":"ok","timestamp":1590104758859,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"},"user_tz":300},"id":"2WeqXo5g00Po","outputId":"1866964b-20b5-4391-fe3d-38f5abe94e0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["looks good!\n"]}],"source":["get_cumulative_rewards(rewards)\n","assert len(get_cumulative_rewards(list(range(100)))) == 100\n","assert np.allclose(get_cumulative_rewards([0, 0, 1, 0, 0, 1, 0], gamma=0.9), [\n","                   1.40049, 1.5561, 1.729, 0.81, 0.9, 1.0, 0.0])\n","assert np.allclose(get_cumulative_rewards(\n","    [0, 0, 1, -2, 3, -4, 0], gamma=0.5), [0.0625, 0.125, 0.25, -1.5, 1.0, -4.0, 0.0])\n","assert np.allclose(get_cumulative_rewards(\n","    [0, 0, 1, 2, 3, 4, 0], gamma=0), [0, 0, 1, 2, 3, 4, 0])\n","print(\"looks good!\")"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CfI4rl0O00Pu"},"source":["#### Loss function and updates\n","\n","We now need to define objective and update over policy gradient.\n","\n","Our objective function is\n","\n","$$ J \\approx  { 1 \\over N } \\sum_{s_i,a_i} G(s_i,a_i) $$\n","\n","\n","Following the REINFORCE algorithm, we can define our objective as follows: \n","\n","$$ \\hat J \\approx { 1 \\over N } \\sum_{s_i,a_i} \\log \\pi_\\theta (a_i \\mid s_i) \\cdot G(s_i,a_i) $$\n","\n","When you compute gradient of that function over network weights $ \\theta $, it will become exactly the policy gradient."]},{"cell_type":"code","execution_count":56,"metadata":{"colab":{},"colab_type":"code","id":"Qegrkl_w00Pw"},"outputs":[],"source":["def to_one_hot(y_tensor, ndims):\n","    \"\"\" helper: take an integer vector and convert it to 1-hot matrix. \"\"\"\n","    y_tensor = y_tensor.type(torch.cuda.\n","                             LongTensor).view(-1, 1)\n","    y_one_hot = torch.zeros(\n","        y_tensor.size()[0], ndims, device=cuda).scatter_(1, y_tensor, 1)\n","    return y_one_hot"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{},"colab_type":"code","id":"XQrz4_hn00P3"},"outputs":[],"source":["# Your code: define optimizers\n","optimizer = torch.optim.Adam(model.parameters(), 1e-3)\n","\n","\n","def train_on_session(states, actions, rewards, gamma=0.99, entropy_coef=1e-2):\n","    \"\"\"\n","    Takes a sequence of states, actions and rewards produced by generate_session.\n","    Updates agent's weights by following the policy gradient above.\n","    Please use Adam optimizer with default parameters.\n","    Confer: https://fosterelli.co/entropy-loss-for-reinforcement-learning\n","    \"\"\"\n","\n","    # cast everything into torch tensors\n","    states = torch.tensor(states, dtype=torch.float32, device=cuda)\n","    actions = torch.tensor(actions, dtype=torch.int32, device=cuda)\n","    cumulative_returns = np.array(get_cumulative_rewards(rewards, gamma))\n","    cumulative_returns = torch.tensor(cumulative_returns, dtype=torch.float32, device=cuda)\n","\n","    # predict logits, probas and log-probas using an agent.\n","    logits = model(states)\n","    probs = nn.functional.softmax(logits, -1)\n","    log_probs = nn.functional.log_softmax(logits, -1)\n","\n","    assert all(isinstance(v, torch.Tensor) for v in [logits, probs, log_probs]), \\\n","        \"please use compute using torch tensors and don't use predict_probs function\"\n","\n","    # select log-probabilities for chosen actions, log pi(a_i|s_i)\n","    log_probs_for_actions = torch.sum(\n","        log_probs * to_one_hot(actions, env.action_space.n), dim=1)\n","\n","   \n","    # Compute loss here. Don't forgen entropy regularization with `entropy_coef` \n","    \"<YOUR CODE>\"\n","    entropy = -torch.sum(log_probs * probs).to(cuda)\n","    #print(entropy)\n","    \"<YOUR CODE>\"\n","    J_hat = torch.mean(log_probs_for_actions * cumulative_returns).to(cuda)\n","    # maximize J value, minimize entropy\n","    loss = - J_hat + entropy_coef * entropy   # this implementation does not use baseline\n","\n","\n","    # Gradient descent step\n","    \"<YOUR CODE>\"\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","    # technical: return session rewards to print them later\n","    return np.sum(rewards)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oWJ-_4D200P9"},"source":["### The actual training"]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":158},"colab_type":"code","executionInfo":{"elapsed":38024,"status":"ok","timestamp":1590104800932,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"},"user_tz":300},"id":"8El-n2iI00P-","outputId":"93106c62-0de6-4674-b788-5ca71c8dda2b"},"outputs":[{"name":"stdout","output_type":"stream","text":["mean reward:12.590\n","mean reward:9.400\n","mean reward:9.320\n","mean reward:9.320\n","mean reward:9.160\n","mean reward:9.260\n","mean reward:9.320\n","mean reward:9.360\n","mean reward:9.410\n","mean reward:9.320\n","mean reward:9.390\n","mean reward:9.310\n","mean reward:9.380\n","mean reward:9.260\n","mean reward:9.420\n","mean reward:9.300\n","mean reward:9.380\n","mean reward:9.330\n","mean reward:9.330\n","mean reward:9.400\n","mean reward:9.320\n","mean reward:9.340\n","mean reward:9.270\n","mean reward:9.350\n","mean reward:9.490\n","mean reward:9.470\n","mean reward:9.240\n","mean reward:9.400\n","mean reward:9.300\n","mean reward:9.270\n","mean reward:9.490\n","mean reward:9.260\n","mean reward:9.370\n","mean reward:9.430\n","mean reward:9.370\n","mean reward:9.420\n","mean reward:9.380\n","mean reward:9.290\n","mean reward:9.290\n","mean reward:9.410\n","mean reward:9.410\n","mean reward:9.280\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[59], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m [train_on_session(\u001b[38;5;241m*\u001b[39mgenerate_session(env))\n\u001b[1;32m      3\u001b[0m                \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)]  \u001b[38;5;66;03m# generate new sessions\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean reward:\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39mmean(rewards)))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(rewards) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m300\u001b[39m:\n","Cell \u001b[0;32mIn[59], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m [train_on_session(\u001b[38;5;241m*\u001b[39m\u001b[43mgenerate_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      3\u001b[0m                \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)]  \u001b[38;5;66;03m# generate new sessions\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean reward:\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (np\u001b[38;5;241m.\u001b[39mmean(rewards)))\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(rewards) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m300\u001b[39m:\n","Cell \u001b[0;32mIn[52], line 16\u001b[0m, in \u001b[0;36mgenerate_session\u001b[0;34m(env, t_max)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Sample action with given probabilities.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<YOUR CODE>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_probs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m new_s, r, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# record session history to train later\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for i in range(100):\n","    rewards = [train_on_session(*generate_session(env))\n","               for _ in range(100)]  # generate new sessions\n","    print(\"mean reward:%.3f\" % (np.mean(rewards)))\n","    if np.mean(rewards) > 300:\n","        print(\"You Win!\")  # but you can train even further\n","        break"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vtofyY_K00QF"},"source":["### Video"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{},"colab_type":"code","id":"Dk3lKedm00QG"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'gym'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Record sessions\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwrappers\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gym\u001b[38;5;241m.\u001b[39mwrappers\u001b[38;5;241m.\u001b[39mMonitor(gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCartPole-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m), directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideos\u001b[39m\u001b[38;5;124m\"\u001b[39m, force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m env_monitor:\n\u001b[1;32m      6\u001b[0m     sessions \u001b[38;5;241m=\u001b[39m [generate_session(env_monitor) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m)]\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gym'"]}],"source":["# Record sessions\n","\n","import gym.wrappers\n","\n","with gym.wrappers.Monitor(gym.make(\"CartPole-v0\"), directory=\"videos\", force=True) as env_monitor:\n","    sessions = [generate_session(env_monitor) for _ in range(100)]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":501,"resources":{"http://localhost:8080/videos/openaigym.video.0.2236.video000064.mp4":{"data":"CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K","headers":[["content-length","1449"],["content-type","text/html; charset=utf-8"]],"ok":false,"status":404,"status_text":""}}},"colab_type":"code","executionInfo":{"elapsed":18683,"status":"ok","timestamp":1590104851830,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"},"user_tz":300},"id":"cbt1WS0o00QM","outputId":"9fd355ea-e6fd-41b8-eb5f-52ea3ce88e32"},"outputs":[{"data":{"text/html":["\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"videos/openaigym.video.0.2236.video000064.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"execution_count":16,"metadata":{"tags":[]},"output_type":"execute_result"}],"source":["# Show video. This may not work in some setups. If it doesn't\n","# work for you, you can download the videos and view them locally.\n","\n","from pathlib import Path\n","from IPython.display import HTML\n","\n","video_names = sorted([s for s in Path('videos').iterdir() if s.suffix == '.mp4'])\n","\n","HTML(\"\"\"\n","<video width=\"640\" height=\"480\" controls>\n","  <source src=\"{}\" type=\"video/mp4\">\n","</video>\n","\"\"\".format(video_names[-1]))  # You can also try other indices"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"colab_type":"code","executionInfo":{"elapsed":9073,"status":"ok","timestamp":1590104867811,"user":{"displayName":"Alfredo Giménez Zapiola","photoUrl":"","userId":"09715893659368711975"},"user_tz":300},"id":"osMNhZ1Z00QQ","outputId":"9e9e6891-b20b-47bf-adb3-a33a35c8c300"},"outputs":[{"name":"stdout","output_type":"stream","text":["Submitted to Coursera platform. See results on assignment page!\n"]}],"source":["from submit import submit_cartpole_pytorch\n","submit_cartpole_pytorch(generate_session, 'agimenezzapiola@gmail.com', 'tZveBlKsoIrQin6Z')"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Ne7v84hm00QW"},"source":["That's all, thank you for your attention!\n","\n","Not having enough? There's an actor-critic waiting for you in the honor section. But make sure you've seen the videos first."]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of practice_reinforce_pytorch.ipynb","provenance":[{"file_id":"https://github.com/yandexdataschool/Practical_RL/blob/coursera/week5_policy_based/practice_reinforce_pytorch.ipynb","timestamp":1590091280284}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
